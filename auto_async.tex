


\begin{comment}
The key of the conversion is to incrementally compute the origin problem and the difference value $\Delta X^k$ of each recursion can be iteratively computed i.e.
 
\begin{equation}
\label{eq:con}
\begin{aligned}
X^{k+1}=&X^k+\Delta X^k\\
\Delta X^{k}=&G' \circ F' (\Delta X^{k-1}).
\end{aligned}
\end{equation}
if aggregate function $G'$ satisfied the \textbf{accumulative} condition, the first line of equation \ref{eq:con} can be rewrite as $X^{k+1}=G'(X^0\cup \Delta X^0\cup \Delta X^1 \ldots \Delta X^k)$.%Note that $G'$ is not necessarily the origin $G$,usually it depends on specific questions.
This is the basic formula of accumulative recursive aggregation which has the same result with normal recursive aggregation .Algorithm can be correctly asynchronized if the \textbf{order-independent} conditions satisfied. Though the basic idea has been proposed,there are still two key problems to be solved. First, how to determine the new aggregation function $G'$. Second,How to determine the incremental value $\Delta X^k$ from $X^k$ and $X^{k+1}$.
Third, how to initialize the incremental value $\Delta X^0$.


Since the new aggregation $G'$ express the relationship between $\Delta X^k$ and $\Delta X^{k-1}$. So the relation can be obtained by evaluate the incremental value between two adjacent recursion. First define the calculation of incremental value as $\Delta X^{k}=diff(X^{k+1},X^{k})$ in which $diff( \cdot , \cdot )$ is a inverse transformation of $G'$, i.e., $X^{k+1}=G'(X^k \cup diff(X^{k+1},X^{k}))$. Then we can try to find a $G'$ satisfied that $diff(G\circ F(X^{k+1}),G \circ F(X^{k})) = G'\circ F(diff(X^{k+1},X^{k}))$. If $G'$ exists and satisfied the community condition, the original algorithm  can be converted. Otherwise, the algorithm can't be converted. 

Usually it is hard to find suitable $G'()$ for an arbitrary algorithm, because it is hard to determine $G'()$ and its inverse transformation $diff()$ at the same time. And even for some aggregate functions there is no inverse transformation to determine the relation between $\Delta X^k$ and $X^k$,$X^{k-1}$.e.q., MIN and MAX. So In this section we only concern about the algorithm with 'SUM' aggregate operations and its variety. Because a large amount of algorithm can be expressed with SUM  operations in their aggregate function. And a lot of them has the ability to accumulatively computing. {\color{red} I want to express that sum is used widely in many algorithm, and even though we only learn the Sum operation, it is still meaningful} For SUM operation and its variety, $diff(X^{k+1},X^{k})$ is the difference between each two adjacent recursion, i.e., subtract each element of the vector correspondingly.

Since we define $\Delta X^k=diff(X^{k+1},X^{k})$,and we have $X^{k+1}=G'(X^0\cup \Delta X^0 \cup G'\circ F'(\Delta X^0)\cup \ldots G' \circ F^n(\Delta X^0))$. To determine $\Delta X^0$,  We can perform normal recursive aggregation for one time and obtain $X^1$. Then $\Delta X^0$ is initialized as $diff(X^1,X^0)$ and $X^0$ is still $X^0$. Then the algorithm can be accumulatively executed.

%\cite{maiter}.
In original PageRank, $R^k=(1-b)AR^{k-1}+b$, where $A$ is a matrix that represents the graph structure. $F$ operation is sending rank value $R_i$ to their outgoing neighbor, corresponding to the multiplication by subscript of transform matrix $A$,$G$ operation is the 'SUM' of intermediate result by the row number of $A$ with constant vector $b$ added. following the define of $diff(\cdot,\cdot)$,
\begin{equation}
\Delta R^k=(1-b)A\Delta R^{k-1}\notag
\end{equation}
So we can obtain that $X^k=X^0+\Delta X^0+(1-b)A\Delta X^0+\ldots+((1-b)A)^{k-1}\Delta X^0$
where $\Delta X^0$ is initialized as $X^1-X^0$. The $F$ operation is matrix multiplication without summing up,and $G$ operation is the 'SUM' of the intermediate result.
Furthermore, $g'$ operation is accumulative,the $f()$ operation and $g'()$ operation in PageRank is order independent($F$,$G'$ is the same), e.q., $\sum_{n}{0.8*x_i/d}=0.8/d*\sum_{n}{x_i}$ which makes it suitable for asynchronous aggregation. There also exist other convertible algorithms, such as Program(8,11,12,
13) in Appendix Sec. \ref{sec:app:example}. 


Next, we formally provide the convertibility conditions as follows.

\begin{theorem}
	\label{th:convert}
	(\textbf{Convertibility}) A recursive program can be converted to an accumulated recursive program for asynchronous aggregation, as long as an aggregate operation $G'()$ with the \textbf{accumulative} and \textbf{commutative} properties can be found with the following conditions:\\
	\begin{itemize}
		\item \textbf{convertible}: $G\circ F(X^{k+1})=G'(X^k\cup G'\circ F(\Delta X^k))$
\item \textbf{eliminable}: $\vert\vert lim_{n\rightarrow\infty}(G'\circ F)^n(x)\vert\vert=\textbf{0}$,
	\end{itemize}
	 The $G'()$ operation is the aggregate operation in the new accumulated recursive program.
\end{theorem}

The formal proof can be found in Appendix Sec. \ref{sec:app:proof:convert}. 
Note that, it is not realistic to perform recursion infinite times. In practice, we will stop the recursion as long as the incremental value $(G'\circ F)^n(x)$ is ``small'' enough.

\end{comment}




\section{Automatic Asynchronization}
\label{sec:async:autoasync}


Though the conditions for asynchronous aggregation have been identified, it is still hard for a non-expert programmer to verify these conditions manually. It is also hard to find a new function $F'()$ for converting a normal recursive program into an accumulated recursive program.
 To alleviate the burden of programmers, an automatic asynchronization scheme is desired. In this  section, we discuss how to automatically verify these conditions given the $g()$ and $f()$ operations\footnote{The $g$ and $f$ operations in Datalog programs can be identified as will be shown in next section.}. To simplify the analysis, we use key-specific operations $g$ and $f$ to describe the automation process instead of using the set oriented operations $G$ and $F$.

The monotonic conditions can be easily checked by the struct of Datalog program which will be detailed in Section 4.1 The other condition verification problems can be thought of as a form of the constraint satisfaction problem, which can be analyzed with the help of \emph{satisfiability modulo theories} (SMT) \cite{53e486195688442995f82bfe28c55731}. The satisfiability of these conditions can then be checked by an SMT solver, such as Z3 \cite{DeMoura:2008:ZES:1792734.1792766}. Next, we show how to reduce these condition verification problems into SMT satisfiability solving problems.

First, the $f()$ and $g()$ functions and the conditions have to be translated into a formula for being verified by an SMT solver. Given a function $H=\{f(x_1),\ldots,f(x_m)\}$ or $H=\{g(x_1),...g(x_n)\}$, we are interested in a formula for $H$ of the form $\phi_H^{o_1,\ldots,o_m}$, where $o_1,\ldots,o_m$ are output variables \cite{Liu:2014:ADP:2670979.2670980}. Intuitively, the formula $\phi$ is ``correct'' if an assignment to $\{x_1,\ldots,x_n,o_1,\ldots,o_m\}$ makes $\phi$ be true if and only evaluating $H(x_1,\ldots,x_n)$ returns $o_1,\ldots,o_m$. Then the satisfiability of $\phi$ exactly implies that the function will compute the same output. Based on this formula, we present the formulas for different conditions in the following.

For the accumulative and commutative conditions in Theorem \ref{th:monotone} and the order independent condition in Theorem \ref{th:async}, the condition verification problems can be reduced to SMT satisfiability problems via the following theorem.

\newtheorem{corollary}{Corollary}
\begin{corollary}
	\label{coro:auto:1}
	(\textbf{Accumulative, Commutative, and Order Independent}) $\forall_{i=1\to m} \{x_{1i},x_{2i},x_{3i}\}$, the condition 1 or 2 or 3 is true if and only if $\phi_{H_l}^{o1,\ldots,o_m}\wedge \phi_{H_r}^{o_1',\ldots,o_m'}\wedge (\vee_{i=1}^m{o_i\neq o_i'})$ is not satisfiable, where
	\begin{itemize}
		\item condition 1: accumulative, $H_l=g(x_1,x_2,x_3)$ and $H_r=g(g(x_1,x_2),x_3)$;
		\item condition 2: commutative, $H_l=g(x_1,x_2)$ and $H_r=g(x_2,x_1)$;
		\item condition 3: order independent, $H_l=g(f(g(x_1,x_2)))$ and $H_r=g(f(x_1),f(x_2))$.
	\end{itemize}
\end{corollary}

Note that, SMT cannot judge ``whether a formula $H$ is always true?'' but only answers ``whether a formula $H$ is satisfiable?''. A formula $H$ is \emph{satisfiable} if there is some assignment of appropriate values to its uninterpreted function and constant symbols under which $H$ evaluates to true. Thus, to verify a condition (that should be always true), we convert the ``$H$ is always true'' problem to the ``not $H$ is not satisfiable'' problem. If $H$ is always true, then ``not $H$'' is always false, and then ``not $H$'' will not have any satisfying assignment


Furthermore, SMT solver Z3 can be used to find a qualified $f'$ function. Following Corollary \ref{coro:auto:1}, we first use Z3 to check the satisfiability.
If the formula is not satisfiable, system will try to find a new $f'$ by the method proposed in sec 3.4. Suppose a new $f'$ is found. If it returns monotonic decreasing values with applying $f'$ more and more times on a real value (i.e., contraction), it can be converted to an accumulated recursive program. Futher, by checking whether $f'$ and $g$ satisfy the order-independent condition, we can determine whether it can be executed asynchronously.
