\section{Automatic Asynchronous Aggregation}
\label{sec:async}
In this section, we first formally define asynchronous aggregation and introduce accumulated recursive program. We then present how to address the problem 

\subsection{Asynchronous Aggregation}
\label{sec:async:async}

The recursive program in Equation (\ref{eq:recursive2}) is defined with synchronous aggregation. In parallel or distributed computing, the \emph{synchronous} aggregation means $F()$ operations have to be completed on each subset before the next $G()$ operation starts. %The $g()$ operation has to be applied on the whole set $X^k$.
Correspondingly, we define asynchronous aggregation as follows.

\begin{definition}
	%wqg
	\label{def:asyncaggre}
	(\textbf{Asynchronous Aggregation}) In a recursive program, we assume that the set $X$  is partitioned into $m$ disjoint subsets,and denote $X_i^k$ i.e., $X=\{X_1^k,X_2^k,\ldots,X_m^k\}$, and $\forall i,j, X_i^k\cap X_j^k=\emptyset$. Asynchronous aggregation is to aggregate multiple subsets from different recursions, i.e., $G(X_1^k \ldots X_i^k\cup X_j^{l}...X_m^k)$ where $k\neq l$.
\end{definition}

From the definition of recursive aggregation as shown in Equation (\ref{eq:recursive2}), $X^{k+1}$ is resulted from $Y^k$, and $Y^k$ is resulted from $X^k$. $X^{k+1}$ does not exist before applying $g()$ on the complete set of $X^k$. Thus, the asynchronous aggregation $g(\ldots X_i^k\cup X_j^{k+1} \ldots)$ is only possible after the $j$th subset of $X^{k+1}$ is obtained. In other words, $X^{k+1}$ is a \emph{replacement} of $X^k$ and is closer to the final result. The asynchronous aggregation of $X^{k+1}$ and $X^k$ may result in a wrong result since $X^k$ is a replaced result which is not supposed to be aggregated.

\subsection{Accumulated Recursive Program}
\label{sec:async:accrec}

In order to make asynchronous aggregation feasible, we introduce a particular type of recursive programs.

\begin{definition}
	\label{def:accumasync}
	(\textbf{Accumulated Recursive Program}) Accumulated recursive program is represented as follows.
	\begin{equation}\label{eq:accumasync}
	\begin{aligned}
	\Delta Y^{k}= F(\Delta X^k)\notag\\
	\Delta X^{k+1}= G(\Delta Y^{k})\notag
	\end{aligned}
	\end{equation}
	It starts from $\Delta X^0=X^0$ and terminates when The 2-norm of $\Delta X^k$ is 0 or small enough. The final result is the aggregation of all intermediate results, i.e., $X^{k+1}=G(\Delta Y^{0} \cup \Delta Y^{1} \cup \ldots \cup \Delta Y^{k})$. Alternatively, the result can be represented as follows.
	\begin{equation}
	\label{eq:accumasyncres}
	G\Big(\Delta Y^0\cup (F\circ G)(\Delta Y^0)\cup\ldots\cup (F\circ G)^k(\Delta Y^0)\Big).
	\end{equation}
\end{definition}

Intuitively, the accumulated recursive program performs the same computation as normal recursive program, but the final result is the aggregation of all the intermediate results as shown in Equation (\ref{eq:accumasyncres}). The asynchronous aggregation becomes meaningful in accumulated recursive programs, since the final result is the \emph{aggregation} of all recursions' results but not the last recursion's result. Note that, $\Delta X^{k}$ represents the \emph{increment} from previous recursion. This implies that the accumulated recursive program exhibits \textbf{monotonicity}, which has been well studied in the literature \cite{Hellerstein:2010:DIE:1860702.1860704,calm,Lam:2013:SDE:2510649.2511289,Wang:2015:AFR:2824032.2824052}. Briefly speaking, monotonicity means that we only add information, never negate or take away. However, not all recursive programs are monotonic. We have the following theorem to guide the transformation.
\begin{theorem}
	\label{th:monotone}
	(\textbf{Monotonizability}) The accumulated recursive program defined in Equation (\ref{eq:accumasync}) will return the same result as the normal recursive program defined in Equation (\ref{eq:recursive2}), as long as the the following conditions are satisfied.
	\begin{itemize}
		\item \textbf{monotonic}: $G(X)\subseteq F\circ G(X)$
		\item \textbf{accumulative}: $G(X_1\cup X_2)=G(G(X_1)\cup X_2)$
		%\item \textbf{commutative}: $g(X_1\cup X_2)=g(X_2\cup X_1)$;
	\end{itemize}
	
\end{theorem}
Here we provide the formal proof :
 \begin{proof}
  It is known that $\Delta Y^k=f(\Delta X^k)$and $\Delta X^0=X^0\\$.
 \begin{align}
 &G\Big(\Delta Y^0\cup (F\circ G)(\Delta Y^0)\cup\ldots\cup (F\circ G)^k(\Delta Y^0)\Big)\notag \\
 =&G\Big(\Delta Y^0\cup F\circ G(\Delta Y^0)\cup \ldots \cup (F\circ G)^n(\Delta Y^0)\Big)    \notag\\
 =&G\Big(G(\Delta Y^0)\cup F\circ G(\Delta Y^0)\cup \ldots \cup (F\circ G)^n( \Delta Y^0)\Big)  \tag{1}\\
 =&G\Big(F\circ G(\Delta Y^0)\cup (F\circ G)^2(\Delta Y^0)\cup \ldots \cup (F\circ G)^n(\Delta Y^0)\Big) \tag{2}\\
 =& \ldots \notag \\
 =&G\Big((F\circ G)^n(Y^0)\Big)\tag{3}\\
 =&(G \circ F)^n(X^0).\notag
 \end{align}
  Line (1) is true because of the accumulative property. Line (2) is true because of the monotonic property. Repeat applying these two property ,we can reduce all the low order items in turn and line(3) can be obtained. 
 \end{proof}

{\color{red} The previous formula is the standard computing formula,but there must be intersection between different recursion.due to the monotonic property.therefore Here we give an incremental computing formula.
	\begin{theorem}
		Define function$f'(x)$ as $f'(x)=f(x)\backslash x$ and likely $F'(X)$.The incremental recursive function defined as follow has the same result with the Normal Recursive program. 
	 \begin{align}
	 	&G(\Delta X^0\cup (F'\circ G)(\Delta X^0)\cup\ldots\cup (F'\circ G)^k(\Delta X^0))\notag 
	 \end{align}
	\end{theorem}
 \begin{proof}
 	It is known that $Y^0=F(X^0)$and $\Delta X^0=X^0\\$. Since $F'(X)\cup X=F(X)$So we have
 	\begin{align}
 	&G(\Delta Y^0\cup (F\circ G)(\Delta Y^0)\cup\ldots\cup (F\circ G)^k(\Delta Y^0))\notag \\
 	=&G(Y^0\cup F'\circ G(Y^0)\cup G(Y^0)\cup \ldots \cup (F'\circ g)^n(Y^0))    \tag{1}\\
 	=&G(Y^0\cup F'\circ G(Y^0)\cup(F'\circ GFG(Y^0) \cup GFG(Y^0)) \ldots \notag \\ &\cup (F\circ G)^n(Y^0)) \tag{2}\\
 	=&G(Y^0\cup F'\circ G(Y^0)\cup (F'\circ G)^2(Y^0)\cup \ldots \cup (F\circ G)^n(Y^0))\notag\\
 	=& \ldots \notag \\
 	=&G(Y^0\cup F'\circ G(Y^0)\cup (F'\circ G)^2(Y^0)\cup \ldots \cup (F'\circ G)^n(Y^0))\notag\\
 	=&(G \circ F)^n(X^0).\notag
 	\end{align}
 	Line(1) is true because of the definition of $F'$,due to the accumulative property, $G(\ldots \cup GFG(Y^0)\cup \ldots)$is equal to $G(\ldots \cup FG(Y^0)\cup \ldots)$, line(2)can be obtain.Then we can easily found that $(F \circ G)^n(Y)=Y \cup (F'\circ G)(Y)\cup \ldots(F' \circ G)^{n-1}$,
 \end{proof}
}
 The accumulative property is essential for both theorem proof and system design. If the accumulative condition is satisfied, only the aggregation result $X^k=G(\Delta X^{0},\ldots,\Delta X^{k-1})$ needs to be maintained and is updated by accumulating a new $\Delta x^{k}$, i.e.$X^{k+1}=G(X^k \cup \Delta X^k)$. Otherwise, the large set $\Delta X^{k}$ needs to be maintained for every recursion $k$.which incur a large storage cost. The accumulative property will be used in our system design to save a lot of maintaining cost.

For example, the SSSP computation (Example 1) can be executed as an accumulated recursive program. Its non-aggregate operation $f()$ on node $i$ outputs not only the tuples set $\{\langle j,td_j^{k+1}\rangle\}$ for its outgoing neighbors $j$ but also its previous recursion's aggregation result $\langle i,d_i^k\rangle$. That is, $\{\langle i,td_i^{k+1}\rangle\}$ and $\langle i,d_i^k\rangle$ are contained in $X^{k+1}$, while $\langle i,d_i^k\rangle$ is already contained in $X^{k}$. Hence, $X^{k}\subseteq X^{k+1}$ and the monotonic condition is satisfied. In addition, the aggregate operation $g()$ which is MIN has the accumulative and commutative conditions.


\subsection{Conditions for Asynchronous Aggregation}
\label{sec:async:condition}

As discussed, asynchronous aggregation is possible in accumulated recursive programs. However, not all accumulated recursive programs with asynchronous aggregation will return the same result as that with synchronous aggregation. We demonstrate the sufficient conditions for asynchronous aggregation in the following theorem.

\begin{theorem}
	\label{th:async}
	(\textbf{Asynchronizability}) With asynchronous aggregation as Definition \ref{def:asyncaggre} proposed, an accumulated recursive program will yield to the same result as with synchronous aggregation, as long as the following order independent condition is satisfied.
	\begin{itemize}
		\item \textbf{order independent}: $g\circ f\circ g(X)=g\circ f(X)$;
		\item \textbf{commutative}: $g(X_1\cup X_2)=g(X_2\cup X_1)$;
	\end{itemize}
\end{theorem}
By asynchronous aggregation, the partial aggregation result is immediately used by the next $f()$ operation. The order independent property implies that no matter the $f()$ operation is first applied or the $g()$ operation is first applied, the effect is the same. As long as the same number of $g()$ operations are applied on all data, the eventual aggregation result will be the same.Due to de definition ,it is obvious that $G$and $F$ operations also satisfied order-independent and commutative properties in this condition. We details the proof in Appendix\ref{sec:app:proof:correct}


For the SSSP example, the $F()$ operation expands the BFS searching scope to one-hop-away nodes, and the $G()$ operation picks the minimal distance resulted from the shortest path. The shortest distances are the same no matter making expansion first $G\circ F(X)$ or making aggregation first $F\circ G(X)$, i.e., $min_j(d_j+w)=min_j(min_j(d_j)+w)$. There are a broad class of computations that satisfy these conditions and can be executed asynchronously (Here we give several examples in Appendix Sec. \ref{sec:app:example}).


\subsection{Conversion Possibilities}
\label{sec:async:convert}

Some non-monotonic computations cannot be written as accumulated recursive program and are not originally qualified for asynchronous aggregation. For example, the PageRank computation cannot be executed as an accumulated recursive program since neither the monotonic condition nor the accumulative condition is satisfied. After applying $F\circ G$ operations, the recursion result $X^{k}$ is replaced by $X^{k+1}$ but not contained in $X^{k+1}$. Further, the aggregate operation $g(\{x_i\})=\sum_i{x_i}+0.15$ does not have the accumulative property due to the additional constant $0.15$.

Fortunately, these computations can be converted to be qualified for asynchronous aggregation. We provide the convertibility conditions as follows.

\begin{theorem}
	\label{th:convert}
	(\textbf{Convertibility}) A recursive program can be converted to an accumulated recursive program for asynchronous aggregation, as long as the recursion performs infinite times and an aggregate operation $g'()$ with the \textbf{accumulative} and \textbf{commutative} properties can be found with the following conditions:
	\begin{itemize}
		\item \textbf{convertible}: $g(X)=g'(X\cup c)$;
		\item \textbf{eliminable}: $lim_{n\rightarrow\infty}(g'\circ f)^n(x)=\textbf{0}$,
\item \textbf{order independent}: $g\circ f\circ g(X)=g\circ f(X)$;
	\end{itemize}
	where the $g'()$ and $f()$ operations are \textbf{order independent} $g'\circ f\circ g'(X)=g'\circ f(X)$, $c$ is a constant value, and \textbf{0} is the identity element of the $g'()$ operation. The $g'()$ operation is the aggregate operation in the new accumulated recursive program.
\end{theorem}

The formal proof can be found in Appendix Sec. \ref{sec:app:proof:convert}. We present the intuition as follows. We aim to find a new aggregate operation $g'()$ that is with an additional constant input value $c$ but always returns the same result as $g()$, i.e., $g(X)=g'(X\cup c)$. The constant input implies the set containment relation between result and input, which makes it satisfy the %monotonic
accumulative condition that is required for accumulated recursive program. In addition, the eliminable property guarantees the % accumulated recursive program to converge, 
the accumulated recursive converge to the same result with recursive aggregation.%such that the result is independent of $X$. 
Note that, it is not realistic to perform recursion infinite times. In practice, we will stop the recursion as long as $(g'\circ f)^n(x)$ is ``small'' enough.

For example, PageRank is such an iterative algorithm that can be converted to the accumulated recursive form %\cite{maiter}. 
In original PageRank, $g(\{x_j\})=\sum_j{x_j}+0.15$. The $g()$ function can be rewritten as $g(X)=g'(X\cup y)$ where the $g'()$ function can be ``SUM'', $X=\{x_j\}$ is the input values set, and the constant value $y$ can be 0.15. Apparently, $g'()$ (i.e., ``SUM'') has the accumulative and commutative property. In addition, the $f()$ operation in PageRank contains a constant damping factor 0.85, which leads to $(g'\circ f)(x)<x$ and $lim_{n\rightarrow\infty}(g'\circ f)^n(x)=0$. Furthermore, the $f()$ operation and $g'()$ operation in PageRank is order independent, which makes it suitable for asynchronous aggregation. The resulted ranking scores are the same no matter making decay first $g\circ f(X)$ or making summation first $f\circ g(X)$, i.e., $\sum_j(0.85*\frac{r_j}{d})=0.85*\frac{\sum_j(r_j)}{d}$. There exist other convertable algorithms, such as Program 13 Jacobi Method in Appendix Sec. \ref{sec:app:example}.


\subsection{Automatic Asynchronization}
\label{sec:async:autoasync}

Though the conditions for asynchronous aggregation have been identified, it is still hard for a non-expert programmer to verify these conditions manually. It is even harder to find an aggregate function $g'()$ in order to convert a normal recursive program to an accumulated recursive program.
 To alleviate the burden of programmers, an automatic asynchronization scheme is desired. In this subsection, we discuss how to automatically verify these conditions given the $g()$ and $f()$ operations\footnote{The $g$ and $f$ operations in Datalog programs can be identified as will be shown in next section.}.

The condition verification problem can be thought of as a form of the constraint satisfaction problem, which can be analyzed with the help of \emph{satisfiability modulo theories} (SMT) \cite{53e486195688442995f82bfe28c55731}. The satisfiability of these conditions can then be checked by an SMT solver, such as Z3 \cite{DeMoura:2008:ZES:1792734.1792766}. Next, we show how to reduce these condition verification problems into SMT satisfiability solving problems.

First, the $f()$ and $g()$ functions and the conditions have to be translated into a formula for being verified by an SMT solver. Given a function $F=f(x_1)$ or $F=g(x_1,\ldots,x_n)$, we are interested in a formula for $F$ of the form $\phi_F^{o_1,\ldots,o_m}$, where $o_1,\ldots,o_m$ are output variables \cite{Liu:2014:ADP:2670979.2670980}. Intuitively, the formula $\phi$ is ``correct'' if an assignment to $\{x_1,\ldots,x_n,o_1,\ldots,o_m\}$ makes $\phi$ be true if and only evaluating $F(x_1,\ldots,x_n)$ returns $o_1,\ldots,o_m$. Then the satisfiability of $\phi$ exactly implies that the function will compute the same output. Based on this formula, we present the formulas for different conditions in the following.

For the accumulative and commutative conditions in Theorem \ref{th:monotone} and the order independent condition in Theorem \ref{th:async}, the condition verification problems can be reduced to SMT satisfiability problems via the following theorem.

\newtheorem{corollary}{Corollary}
\begin{corollary}
	\label{coro:auto:1}
	(\textbf{Accumulative, Commutative, and Order Independent}) $\forall x_1,x_2,x_3$, the condition 1 or 2 or 3 is true if and only if $\phi_{F_l}^{o1,\ldots,o_m}\wedge \phi_{F_r}^{o_1',\ldots,o_m'}\wedge (\vee_{i=1}^m{o_i\neq o_i'})$ is not satisfiable, where
	\begin{itemize}
		\item condition 1: accumulative, $F_l=g(x_1,x_2,x_3)$ and $F_r=g(g(x_1,x_2),x_3)$;
		\item condition 2: commutative, $F_l=g(x_1,x_2)$ and $F_r=g(x_2,x_1)$;
		\item condition 3: order independent, $F_l=g(f(g(x_1,x_2)))$ and $F_r=g(f(x_1),f(x_2))$.
	\end{itemize}
\end{corollary}

In the theorem, $\wedge$ denotes AND operator and $\vee$ denotes OR operator. Note that, SMT solver cannot answer ``whether a formula $F$ is always true?'' but only answers ``whether a formula $F$ is satisfiable?''. A formula $F$ is \emph{satisfiable} if there is some assignment of appropriate values to its uninterpreted function and constant symbols under which $F$ evaluates to true. Thus, to verify a condition (that should be always true), we convert the ``$F$ is always true'' problem to the ``not $F$ is not satisfiable'' problem. If $F$ is always true, then ``not $F$'' is always false, and then ``not $F$'' will not have any satisfying assignment

here need new description


Furthermore, SMT solver Z3 can be used to find a qualified aggregate function $g'()$ in Theorem \ref{th:convert}. We declare a constant $c$ and declare a function $g'()$ without defining the specific computation. Following Theorem \ref{th:auto:4}, we use Z3 to check the satisfiability. If the formula is satisfiable, it will return the possible $g'()$ and $c$, otherwise it will return unknown or unsatisfiable. We can then convert the returned $g'()$ to a program to achieve automatic conversion.
