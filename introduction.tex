\section{Introduction}
Iterative algorithms or recursive algorithms widely exist in data analytics fields. With the increasing of sensor performance and the rapid expansion of social network data, it is necessary to parallelize or distribute the iterative computation of large scale data. However, the parallelization of algorithms requires programmers to have a rich knowledge of distributed systems. A large number of big data analysis systems have been proposed which provide high-level language for expressing the recursive computation logic \cite{Dean:2004:MSD:1251254.1251264,giraph,maiter,Fan:2017:PSG:3035918.3035942,Malewicz2010Pregel,DBLP:journals/corr/GonzalezBJFHGS15,8017445,Low:2012:DGF:2212351.2212354,Han:2015:GUB:2777598.2777604,grace}. Even so, these systems still require the users to understand the specific programming logic. For example, Pregel \cite{Malewicz2010Pregel} adopts the Bulk Synchronous Parallel (BSP) computing model and a vertex-centric programming model, which requires programmers to write vertex-centric programs and explicitly manage the communication.

\begin{comment}
{\color{red}
New interest has recently re-emerged around Datalog for a wide spectrum of knowledge-oriented applications \cite{Aref:2015:DIL:2723372.2742796,7840589,Shkapsky:2013:GQN:2536274.2536290,Alvaro:2010:DDT:2185923.2185942,Shkapsky:2016:BDA:2882903.2915229,Lam:2013:SDE:2510649.2511289,Seo:2013:DSD:2556549.2556572,Wang:2015:AFR:2824032.2824052}%Bigdatalog 25,27,64,47,49}.\begin{comment}
{\color{red}Datalog is an excellent candidate language for large-scale data analytics because of its high-level declarative semantics and support for recursion. Datalog's support for recursion makes the expression of data analysis natural \cite{}{\color{red}which paper}  and its high-level semantics makes it amenable to parallelization and optimization \cite{}{\color{red}which paper}.

In recent years, many system research efforts have raised to improve performance  and scalability based on Datalog systems. Socialite \cite{Lam:2013:SDE:2510649.2511289,Seo:2013:DSD:2556549.2556572} provides a large scale graph evaluation system supporting both sequential and distribute environment. In Socialite, users can define recursive aggregate functions which, as long as they are meet operations, can be evaluated incrementally and efficiently. The \cite{7113340} project provides a full Datalog language implementation and seeks to provide  system supports that optimizes execution over diverse platforms including sequential implementations \cite{Shkapsky:2016:BDA:2882903.2915229}, multi-core machines, and clusters \cite{Shkapsky:2016:BDA:2882903.2915229}. It supports relational algebra, aggregation, and recursion, as well as a host of declarative optimizations. MyriaX \cite{Halperin:2014:DMB:2588555.2594530} implements a Datalog System on share-nothing engines based on Myria \cite{Halperin:2014:DMB:2588555.2594530}. The computations are incremental, and it support s a variety of iterative models (synchronous, asynchronous, different processing priorities) and failure-handling techniques. It is worth mentioning that MyriaX supports asynchronous processing and shows promising performance for some applications, but it fails to tell in which cases asynchronous processing is suited.
}
\end{comment}

Synchronous processing and asynchronous processing are two basic parallel execution strategies for recursive processing. Many synchronous processing based parallel/distributed systems \cite{Malewicz2010Pregel,Dean:2004:MSD:1251254.1251264,giraph,maiter,Fan:2017:PSG:3035918.3035942,Malewicz2010Pregel,8017445,Low:2012:DGF:2212351.2212354} as well as asynchronous systems \cite{Low:2012:DGF:2212351.2212354, Tian:2013:TLV:2732232.2732238, Han:2015:GUB:2777598.2777604, grace} have emerged in recent years. Compared with synchronous recursive processing, asynchronous recursive processing has many advantages, such as fast convergence \cite{maiter}, more efficient resource utilization \cite{priori-cidr}, and the ability of using priority scheduling \cite{Zhang:2011:PDF:2038916.2038929}. Even though asynchronous processing is not always the best choice for all workloads (it does not consistently show better performance over synchronous processing) \cite{Fan2018Adaptive,Xie2015SYNC} ,  asynchronous processing is a more general processing model, and synchronous processing can be considered as a special case of asynchronous processing with synchronous scheduling \cite{Fan2018Adaptive}. 

However in practice, synchronous systems are more popular than asynchronous systems, which can be mainly attributed to the following two points:

First, \textbf{Non-guaranteed Correctness}. There are several prior works that have employed asynchronous processing engine for improving their system performance. However, these systems only implement asynchronous execution of parallel/distributed algorithms, without any correctness guarantee of the results \cite{Low:2012:DGF:2212351.2212354}. Asynchronous computation model are blindly used and may result in inconsistent results for convex functions. Maiter \cite{maiter} provides the sufficient conditions for correct asynchronous computations, but these conditions are only suitable in the context of vertex-centric graph computations, and programmers should judge the conditions manually by themselves.

%Second,\textbf{Ununified computing expression}.
%Recursive Algorithm usually has ununified computing expression.Even if the correctness condition are given,it is still difficult to check it from variety handwrite program with different style. Not to say convert the normal program into asynchronous formulation.It seems that we need an high-declarative language to express the computing logic.

Second, \textbf{More Complexity}. Programmers are used to write sequential programs or synchronous parallel programs. Had the asynchronization conditions been formally given, it is still hard for a non-expert programmer to manually verify these conditions from their programs. In addition, writing asynchronous programs and debugging asynchronous systems are even harder, because asynchronous implies disorganized and as a result complicated. Experience from Google \cite{Malewicz2010Pregel} strongly suggests a synchronous programming model, since asynchronous code is a lot harder to write, tune, and debug.

%Third, \textbf{More Complexity}. Programmers are used to write sequential programs or synchronous parallel programs. So writing asynchronous programs and designing asynchronous systems are even harder, because asynchronous implies disorganized and as a result complicated. Experience from Google \cite{}{\color{red}which paper} strongly suggests a synchronous programming model, since asynchronous code is a lot harder to write, tune, and debug.

%{\color{red}
%Third, \textbf{Unstable Performance}. Asynchronous iterative processing avoids the intermediate result coordination phase. The parallel executions of operations are not synchronized and not strictly ordered. This implies that the computations and communications are not under control any more, which may lead to stale computations/communications and potentially reduces the efficiency. The performance gain from asynchronous computation may be not enough to compensate for the performance loss from stale computations/commmunications, leading to unstable performance.
%}

To address the first problem, we seek to find the root reason that causes wrong results. In parallel computing, the aggregation operation that is contained in a recursive program needs to be synchronous, i.e., the aggregate operation should wait for all its inputs to be ready before starting. Otherwise, the aggregation result is wrong, so is the final result. We observe that a broad class of recursive algorithms with accumulated (or monotonic) recursive aggregation have the possibility of returning the correct results. For example, CC, SSSP, computing paths in DAG,... algorithms are accumulated recursive programs. Given an accumulated recursive program that is composed of interleaving aggregate and non-aggregate operations, as long as the aggregate operation has the commutative property and the aggregate operation and non-aggregate operation have order-independent property, it will return the same result with synchronous processing after enough number of updates. A large class of recursive programs can be rewritten in an accumulated/monotonic form. Even though some cannot, we provide the conditions for converting them. For instance, Pagerank, Belief Propagation, Simrank, Jacobi Method... algorithms that are not originally in the monotonic form can be converted.

To address the second problem, we propose an asynchronization strategy that can automatically translate a user-specified Datalog recursive program to a asynchronous parallel Java program running on multi-core machine or cluster. We choose Dalalog as the high level language because of its high-level declarative semantics and support for parallel recursion \cite{Shkapsky:2016:BDA:2882903.2915229}. The aggregate operation and non-aggregate operation can be extracted from user's Datalog program. These abstract operations are automatically checked for verifying whether this recursive program satisfies asynchronous execution conditions. The automatic verification is achieved with the help of a Satisfactory Module Theory (SMT) solver Z3 \cite{DeMoura:2008:ZES:1792734.1792766}. 

%Our system is able to automatically check whether a recursive Datalog program can return correct result with asynchronous recursive execution from user's Datalog program. This is achieved by automated sufficient conditions verification using Z3 smt solver. 
%A3Log can also automate the conversion for some algorithm that satisfied the convertible condition without user's participation. Further, A3Log provides both shared-memory runtime engine and distributed runtime engine for fast execution.


%We propose the correctness conditions based on the analysis of aggregate and non-aggregate operations.

%n an iterative computation can be abstracted into a series of non-aggregate and aggregate operations.Aggregate operation is known to be hardly parallelized{\color{red} which paper} \cite{distribute aggregate from ms}, while non-aggregate operation can be embarrassingly parallel.

%First,we abstract the iterative computation into recursive aggregation program. And then we  Then we proposed accumulate recursive aggregation and defined the conversion conditions from normal recursive aggregations. Then we generalized the Further some unsatisfiable algorithm, can even be automatically converted to the asynchronous program with our convertible schema.

%we also design and implement a Datalog system supporting automated asynchronous execution, A3Log.We choose Dalalog as its high level language because of its high-level declarative semantics and support for parallel recursion{\color{red}which paper}\cite{}. Our system is able to automatically check whether a recursive Datalog program can return correct result with asynchronous recursive execution from user's Datalog program. This is achieved by automated sufficient conditions verification using Z3 smt solver. 
%A3Log can also automate the conversion for some algorithm that satisfied the convertible condition without user's participation. Further, A3Log provides both shared-memory runtime engine and distributed runtime engine for fast execution.


%Maiter \cite{maiter} provides the sufficient conditions for asynchronous graph processing,but these conditions are not generalized enough and only suit for several graph algorithm. In this paper, we generalize the sufficient conditions which can be easily identified from  user's Datalog program. %Aggregate operation is known to be hardly parallelized \cite{distribute aggregate from ms}, while non-aggregate operation can be embarrassingly parallel.% Synchronization of parallel computations seems to be the unavoidable coordination step for correct aggregation result though it is known costly. 
The contributions of this paper are summarized as follows.
\begin{itemize}
	%{\color{red}\item To guarantee the correctness of recursive Datalog program, we proposed a series of conditions of converting  recursive aggregation into a self-defined \textbf{Accumulative Recursive AgGregation}. Moreover new sufficient conditions that guarantee asynchronous processing to return the same result as synchronous processing. which has proposed based on the analysis  of aggregate and non-aggregate operators in ARAG.Even for some recursive programs that do not satisfy these conditions, we propose an approach to conditionally convert them to be qualified for asynchronous aggregation.}
	\item We define the accumulated recursive aggregation (ARA) that has the possibility of returning correct results when executing asynchronously. We also propose the convert conditions from normal recursive aggregation (NRA) to ARA. Furthermore, based on the analysis of aggregate and non-aggregate operations' properties, we provide the conditions for correctly converging to the result when asynchronously executing ARA.  
	\item To alleviate the burden of programmers, we propose an automated condition verification technique by leveraging satisfiability modulo theories (SMT). User's accumulated recursive program (ARP) can be automatically checked for asynchronous execution possibilities and can even be automatically translated to an asynchronous parallel program.
	\item We propose a Datalog implementation, A3Log, to support automated asynchronous aggregation. A3Log is built by modifying distributed Socialite \cite{Seo:2013:DSD:2556549.2556572}. The condition verification techniques are embedded in a Condition Checker component, so that it can asynchronize user's program automatically. A3Log provides both shared-memory runtime engine and distributed runtime engine. The evaluation of a Datalog program are executed updating a distributed hash table structure. A3Log also offers numerous optimizations and functionalities to support high performance iterative computations, such as concurrency control, priority scheduling, and termination control.
	\item We experimentally evaluate A3Log by comparing with Socialite\cite{Lam:2013:SDE:2510649.2511289} , GraphLab\cite{Low:2012:DGF:2212351.2212354},  Myria\cite{Wang:2015:AFR:2824032.2824052} and Maiter\cite{maiter}. We also perform evaluations with 7 algorithms on 20 more datasets. The experiments are performed on a 32-core instance for many-core experiments and on a cluster with 64 instances for distributed experiments. Our results show that A3Log outperforms other systems in many-core experiments and shows comparable performance with Maiter in distributed experiments. Our results also show that the asynchronous execution of A3Log can achieve 2.25X-222.82X speedup over the synchronous version for various datasets.
\end{itemize}

The rest of the paper are organized as follow: In Sec.2 we describe the details of  automatic asynchronous technology. In Sec.3 we propose a datalog system based on automatic asynchronous the technology. In Section.4  we give the performance evaluation. And then we review the related works in Sec 5 and then conclude the paper in Sec.7.


